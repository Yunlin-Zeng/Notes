{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b272ea10-6245-4032-89f2-7b9d2860f370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d158e2-23d2-47dc-bd4b-abb830b27ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yunlin/bin/miktex\n"
     ]
    }
   ],
   "source": [
    "!which miktex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ddcdc1-4662-44a6-9922-08f8bc9ded7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) (preloaded format=pdflatex)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "(./test.tex\n",
      "LaTeX2e <2021-11-15> patch level 1\n",
      "L3 programming layer <2022-02-24>\n",
      "(/usr/local/texlive/2022/texmf-dist/tex/latex/base/article.cls\n",
      "Document Class: article 2021/10/04 v1.4n Standard LaTeX document class\n",
      "(/usr/local/texlive/2022/texmf-dist/tex/latex/base/size10.clo))\n",
      "(/usr/local/texlive/2022/texmf-dist/tex/latex/l3backend/l3backend-pdftex.def)\n",
      "(./test.aux) [1{/usr/local/texlive/2022/texmf-var/fonts/map/pdftex/updmap/pdfte\n",
      "x.map}] (./test.aux) )</usr/local/texlive/2022/texmf-dist/fonts/type1/public/am\n",
      "sfonts/cm/cmr10.pfb>\n",
      "Output written on test.pdf (1 page, 13411 bytes).\n",
      "Transcript written on test.log.\n"
     ]
    }
   ],
   "source": [
    "!echo \"\\documentclass{article}\\begin{document}Hello, MikTeX!\\end{document}\" > test.tex\n",
    "!pdflatex test.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d7b7592-7ebf-4017-9e50-98bff9950a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/texlive/2022/texmf-dist/tex/latex/physics/physics.sty\n"
     ]
    }
   ],
   "source": [
    "!kpsewhich physics.sty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb80389-a384-40ae-b633-563081c41288",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\\usepackage{physics}$\n",
    "\n",
    "\n",
    "$\\pdv{f}{x}$\n",
    "\n",
    "\\vqty{a}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332b878-b345-4c7a-8f87-b0d58a1d97cb",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1nEjlDpHc1Seb7U2tu4A2Y-5w0GB6DTht?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953da9a5-d948-49fe-b6c3-e73bc23ea940",
   "metadata": {},
   "source": [
    "Notes are written based on [Berkeley CS 285](https://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-18.pdf) and [Rezende 2015 Variational Inference with Normalizing Flows](https://arxiv.org/abs/1505.05770)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd3b0e-2062-4069-b051-a9b02462f737",
   "metadata": {},
   "source": [
    "<img src=\"presentation_img/abstract.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d35ef-c4bc-4f69-aef1-016490149df0",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0492529-a3c6-4534-97bd-be70b8f344bf",
   "metadata": {},
   "source": [
    "#### Latent variable model\n",
    "\n",
    "* Probabilistic models, $p(x)$, $p(y|x)$.\n",
    "* Latent variables are neither evidence nor query.\n",
    "* A classical model is mixture Gaussian, where the latent variable is $z = 1, 2, 3$, and $p(x) = \\underset{z}{\\sum} p(x|z)p(z)$.\n",
    "<img src=\"presentation_img/CS285_lec18_part1_0.png\" width=\"400\">\n",
    "\n",
    "* How to train latent variable model? We want a maximum likelihood fit\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta \\leftarrow \\underset{\\theta}{\\text{argmax}} \\frac{1}{N}\\sum_i\\log{p_\\theta(x_i)},\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "    p(x) = \\int p(x|z)p(z)\\,dz.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{gather}\n",
    "    \\Rightarrow \\theta \\leftarrow \\underset{\\theta}{\\text{argmax}} \\frac{1}{N}\\sum_i\\log{\\left(\\int p_\\theta(x|z)p(z)\\,dz\\right)} \n",
    "    \\label{eq:train-latent-var}\n",
    "\\end{gather}\n",
    "\n",
    "* But the Equation \\eqref{eq:train-latent-var} completely intractable especially in continuous models. So instead, we try to estimate the expected log-likelihood,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta \\leftarrow \\underset{\\theta}{\\text{argmax}} \\frac{1}{N}\\sum_i E_{z\\sim p(z|x_i)}[\\log{p_\\theta (x_i, z)}]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee0604-8c4e-473f-bca0-e08f63ae0edd",
   "metadata": {},
   "source": [
    "#### The variational approximation\n",
    "\n",
    "* How do we calculate $p(z|x_i)$? What if we can approximate with $q_i(z) = \\mathcal{N}(\\mu_i, \\sigma_i)$, where $q_i$ is some simple distribution.\n",
    "* With a $q_i$, you can construct a lower bound on $\\log{p(x_i)}$. Then, maximizing this bound will push up $\\log{p(x_i)}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\log{p(x_i)} &= \\log{\\int_z p(x_i|z)p(z)} \\nonumber \\\\\n",
    "    &= \\log{\\int_z p(x_i|z)p(z)\\frac{q_i(z)}{q_i(z)}} \\nonumber \\\\\n",
    "    &= \\log{E_{z\\sim q_i(z)}\\left[\\frac{p(x_i|z)p(z)}{q_i(z)}\\right]} \\nonumber \\\\\n",
    "    &\\ge E_{z\\sim q_i(z)}\\left[\\log{\\frac{p(x_i|z)p(z)}{q_i(z)}}\\right] \\quad \\text{use Jensen's inequality for concave functions (log)} \\log{E[y]} \\ge E[\\log{y}] \\nonumber\\\\\n",
    "    &= E_{z\\sim q_i(z)}[\\log{p(x_i|z)} + \\log{p(z)}] - E_{z\\sim q_i(z)}[\\log{q_i(z)}] \\quad \\text{equation stands for all $q_i$} \\nonumber \\\\\n",
    "    &= E_{z\\sim q_i(z)}[\\log{p(x_i|z)} + \\log{p(z)}] - \\mathcal{H}(q_i(z)) \\quad \\text{last term is \"entropy\"} \\nonumber \\\\\n",
    "    &= E_{z\\sim q_i(z)}[\\log{p(x_i|z)}] - D_{\\rm KL}(q_i(z)||p(z)) \\label{eq:ELBO1} \\quad \\text{last term is KL divergence} \\\\\n",
    "    &= \\mathcal{L}_i \\label{eq:Li},\n",
    "\\end{align}where $\\mathcal{L}_i$ is called the evidence lower bound (ELBO).\n",
    "\n",
    "#### A brief aside of entropy\n",
    "\n",
    "* $\\mathcal{H}$ is the entropy, \n",
    "\\begin{equation}\n",
    "    \\mathcal{H(q)} = -E_{x\\sim q(x)}[\\log{q(x)}] = -\\int_x q(x)\\log{q(x)} \\,dx\n",
    "\\end{equation}\n",
    "    * Intuition 1: how random is the random variable?\n",
    "    * Intuition 2: how large is the log probability in expectation under itself?\n",
    "    \n",
    "#### A brief aside of KL divergence\n",
    "\n",
    "* A brief aside of KL divergence \n",
    "\\begin{equation}\n",
    "    D_{\\rm KL}(q(z)||p(z)) = E_{z\\sim q(z)}\\left[\\log{\\frac{q(z)}{p(z)}}\\right] = E_{x\\sim q(z)}[\\log{q(z)}] - E_{z\\sim q(z)}[\\log{p(z)}] = -E_{z\\sim q(z)}[\\log{p(z)}] - \\mathcal{H}(q)\n",
    "\\end{equation}\n",
    "    * Intuition 1: how different are two distributions?\n",
    "    * Intuition 2: how small is the expected log probability of one distribution under another, minus entropy?\n",
    "* KL divergence always $\\ge 0$.\n",
    "    * Derivation ...\n",
    "\n",
    "\n",
    "#### The variational approximation (cont.)\n",
    "\n",
    "* Equations \\eqref{eq:ELBO1} and \\eqref{eq:Li} suggest that we maximize $\\mathcal{L}_i(p, q_i)$.\n",
    "* On the other hand,\n",
    "\\begin{align}\n",
    "    &... \\text{leave out derivation (slides 15, 16)} \\nonumber \\\\\n",
    "    \\Rightarrow \\quad & D_{\\rm KL}(q_i(z)||p(z|x_i)) = -\\mathcal{L}_i (p, q_i) + \\log{p(x_i)} \\label{eq:algorithm}\n",
    "\\end{align}\n",
    "* Note the $D_{\\rm KL}$ terms of Equations \\eqref{eq:ELBO1} and \\eqref{eq:algorithm} are different, but the $\\mathcal{L}_i$ terms are the same.\n",
    "* <font color=blue>The point is Equation \\eqref{eq:algorithm} immediately suggests a practical learning algorithm: Take the variational lower bound, maximize w.r.t. $q_i$ to get the tightest bound (also minimize KL divergence), and then maximize w.r.t. $p$ to improve the likelihood. Then alternate these two steps.</font>\n",
    "    * Our goal was $\\theta \\leftarrow \\underset{\\theta}{\\text{argmax}} \\frac{1}{N}\\sum_i \\log{p_\\theta (x_i)}$ but it is intractable, so instead we do $\\theta \\leftarrow \\underset{\\theta}{\\text{argmax}} \\frac{1}{N}\\sum_i \\log{\\mathcal{L}_i(p, q_i)}$\n",
    "    * For each $x_i$ (or mini-batch):\n",
    "        * Calculate $\\nabla_\\theta \\mathcal{L}_i (p, q_i)$\n",
    "            * Sample $z \\sim q_i(z)$ \n",
    "            * $\\nabla_\\theta \\mathcal{L}_i (p, q_i) \\approx \\nabla_\\theta\\log{p_\\theta}(x_i|z) \\quad$ Use the sample to estimate the gradient.\n",
    "        * Improve $\\theta$, $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta\\mathcal{L}_i(p, q_i)$\n",
    "        * Update $q_i$ to maximize the same evidence lower bound $\\mathcal{L}_i (p, q_i) \\leftarrow$ How to do this? \n",
    "        \n",
    "* In the last step, let's say $q_i(z) = \\mathcal{N}(\\mu_i, \\sigma_i)$, and it's possible to do $\\nabla_{\\mu_i} \\mathcal{L}$ and $\\nabla_{\\sigma_i} \\mathcal{L}$ to optimize $\\mu_i, \\sigma_i$. But it's not scalable for large dataset. There are in total $|\\theta| + (|\\mu_i| + |\\sigma_i|)\\times N$ parameters.\n",
    "\n",
    "\n",
    "\n",
    "* To execute the algorithm above effectively, we need to solve the following two problems:\n",
    "    * Efficient computation of the derivative of the first term of Equation \\eqref{eq:ELBO1}: $\\nabla_\\phi E_{z\\sim q_\\phi(z|x_i)}[\\log{p_\\theta(x_i|z)}]$.\n",
    "        * What about the KL divergence term? <font color=red>If you assume $q_i(z)$ and $p(z)$ both to be Gaussian, then the KL divergence is analytically known.</font>\n",
    "    * Choosing the richest, computationally-feasible approximation of posterior distribution $q$. \n",
    "* Solutions:\n",
    "    * Problem 1: Amortized variational inference $\\rightarrow$ Monte Carlo gradient estimate + inference networks.\n",
    "    * Problem 2: Normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3b690-5499-48cd-a23d-a0a6dcb5446b",
   "metadata": {},
   "source": [
    "## Amortized variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42024a83-f550-46b8-9592-6f4718f76767",
   "metadata": {},
   "source": [
    "* Instead of having a separate $q_i(z)$ for every data point $x_i$, we have one network $q(z|x_i)$, which aims to approximate posterior.\n",
    "* Then in our generative model, we have one network that maps from $z$ to $x$ ($p_\\theta(x|z)$), and another network that maps from $x$ to $z$ ($q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi(x))$). That's the idea behind amortized variational inference.\n",
    "\n",
    "<img src=\"presentation_img/CS285_lec18_part3_network.png\" width=\"700\">\n",
    "\n",
    "* The ELBO becomes \n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_i = \\mathcal{L}(p_\\theta(x_i|z), q_\\phi(z|x_i)) = E_{\\color{brown}{z\\sim q_\\phi(z|x_i)}}[\\log{p_\\theta(x_i|z)}] - D_{\\rm KL}(\\color{brown}{q_\\phi(z|x_i)}||p(z)) \\label{eq:ELBO2}. \n",
    "\\end{equation}\n",
    "* The algorithm above becomes \n",
    "    * For each $x_i$ (or mini-batch):\n",
    "        * Calculate $\\nabla_\\theta \\mathcal{L}(p_\\theta(x_i|z), q_\\phi(z|x_i))$\n",
    "            * Sample $z \\sim \\color{brown}{q_\\phi(z|x_i)}$\n",
    "            * $\\nabla_\\theta \\mathcal{L}_i (p, q_i) \\approx \\nabla_\\theta\\log{p_\\theta}(x_i|z)$\n",
    "        * $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta\\mathcal{L}_i(p, q_i)$\n",
    "        * $\\color{brown}{\\phi \\leftarrow \\phi + \\alpha \\nabla_\\phi\\mathcal{L}_i(p, q_i)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12aa932-e9eb-4d1a-9ac3-4a4796400222",
   "metadata": {},
   "source": [
    "#### Reparameterization trick\n",
    "\n",
    "* Parameterize $z = \\mu_i + \\epsilon \\sigma_i$, and draw $\\epsilon\\sim \\mathcal{N}(0, 1)$.\n",
    "* <font color=red>In reinforcement learning, we cannot calculate the derivatives through dynamics. But in amortized VI, there's no unknown dynamics. There's only $q$. And calculate derivatives through mean and variance of $q$ is feasible.</font>\n",
    "* Express $z$ as a deterministic function, parameterized by $\\phi$, of another random variable $\\epsilon$ that is independent of $\\phi$. <font color=red>When we do this, we can get a better gradient estimate. ...... $\\phi$ here only parameterizes a deterministic quantity. We can write a better gradient estimator based on this expectation of $\\epsilon$.</font>\n",
    "* The ELBO\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_i &= E_{z\\sim q_\\phi(z|x_i)}[\\log{p_\\theta(x_i|z)}] - D_{\\rm KL}(q_\\phi(z|x_i)||p(z)) \\label{eq:ELBO3} \\\\\n",
    "                  &= E_{z\\sim \\mathcal{N}(0, 1)}[\\log{p_\\theta(x_i|\\mu_\\phi(x_i)) + \\epsilon\\sigma_\\phi(x_i)}] - D_{\\rm KL}(q_\\phi(z|x_i)||p(z)) \\\\\n",
    "                  &\\approx \\log{p_\\theta}(x_i|\\mu_\\phi + \\epsilon\\sigma_\\phi(x_i)) - D_{\\rm KL}(q_\\phi(z|x_i)||p(z)) .\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"presentation_img/CS285_lec18_part3_repara_network.png\" width=\"700\">\n",
    "\n",
    "* Why the ELBO (Equation \\eqref{eq:ELBO3}) works well?\n",
    "    * Reconstruction term and KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e40321-e7c2-4e6d-adcb-1e4a4460e029",
   "metadata": {},
   "source": [
    "## Normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3f9cc-614f-4032-aa3c-57ec7f752636",
   "metadata": {},
   "source": [
    "* In Equation \\eqref{eq:ELBO3}, the optimal variational distribution allows $D_{\\rm KL}(q||p) = 0$, which means $q_\\phi(z|x) = p_\\theta(z|x)$. \n",
    "* But this is impossible as generally we assign $q$ a Gaussian or other mean-field approximation.\n",
    "* Normalizing flow allows one to obtain variational distributions that are highly flexible and preferably close to the true posterior distribution, by transforming a probability density through a sequence of invertible mappings.\n",
    "\n",
    "\\begin{equation}\n",
    "    q(z') = q(z) \\left|\\det{\\frac{\\partial f^{-1}}{\\partial{z}'}}\\right| = q(z)\\left|\\det{\\frac{\\partial f}{\\partial{z}}}\\right| ^{-1} \\label{eq:transform-q}\n",
    "\\end{equation}\n",
    "\n",
    "* We can achieve arbitrarily complex densities by successively applying simple maps through Equation \\eqref{eq:transform-q}.\n",
    "* After $K$ times of transformations\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ln{q_K(\\mathbf{z}_K)} = \\ln{q_0(\\mathbf{z}_0)} - \\sum_{k=1}^K \\ln{\\left|\\frac{\\partial f_k}{\\partial{\\mathbf{z}_{k-1}}}\\right|},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}_K = f_K \\circ ... \\circ f_2 \\circ f_1(\\mathbf{z}_0)\n",
    "\\end{equation}\n",
    "* The initial distribuiton $q_0(\\mathbf{z}_0)$ is called the flow, and the path formed by distributions $q_k$ is a normalizing flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25adc7-39d9-42cc-9f5c-18982496bd8d",
   "metadata": {},
   "source": [
    "Example: [Chatterjee 2022 Pre-merger sky localization of gravitational waves from binary neutron star mergers using deep learning](https://arxiv.org/abs/2301.03558)\n",
    "\n",
    "<img src=\"presentation_img/model_architect.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45321a43-62ad-48b7-92bd-dba86aac8b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skymap",
   "language": "python",
   "name": "skymap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
